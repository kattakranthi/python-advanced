from pyspark.sql import SparkSession
import pandas as pd 
# Create SparkSession
spark = SparkSession.builder.getOrCreate()

# Load employee and department DataFrames
employee_df = spark.read.csv('employee.csv', header=True, inferSchema=True)
department_df = spark.read.csv('department.csv', header=True, inferSchema=True)

# Perform inner join based on department ID
joined_df = employee_df.join(department_df, employee_df['department_id'] == department_df['department_id'], 'inner')

# Display the joined DataFrame
joined_df.show()

//Join using pandas
employee_df_pandas = pd.read_csv('employee.csv')
department_df_pandas = pd.read_csv('department.csv')


# Create DataFrames
data1 = [(1, "Alice"), (2, "Bob"), (3, "Charlie")]
data2 = [(1, "Engineer"), (2, "Manager"), (4, "Director")]

columns1 = ["id", "name"]
columns2 = ["id", "role"]

df1 = spark.createDataFrame(data1, columns1)
df2 = spark.createDataFrame(data2, columns2)

# Show DataFrames
print("DataFrame 1:")
df1.show()

print("DataFrame 2:")
df2.show()

# Left Join
left_join_result = df1.join(df2, on="id", how="left")
print("Left Join Result:")
left_join_result.show()

# Right Join
right_join_result = df1.join(df2, on="id", how="right")
print("Right Join Result:")
right_join_result.show()

# Stop Spark session
spark.stop()

