from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# Create SparkSession
spark = SparkSession.builder.getOrCreate()

# Load employee DataFrame
employee_df = spark.read.csv('employee.csv', header=True, inferSchema=True)

# Display the employee DataFrame
employee_df.show()

# Count the number of employees
employee_count = employee_df.count()
print("Employee Count:", employee_count)

# Calculate the average salary
average_salary = employee_df.select(avg('salary')).first()[0]
print("Average Salary:", average_salary)

# Calculate the maximum salary
max_salary = employee_df.select(max('salary')).first()[0]
print("Maximum Salary:", max_salary)

# Calculate the minimum salary
min_salary = employee_df.select(min('salary')).first()[0]
print("Minimum Salary:", min_salary)

# Calculate the sum of salaries
total_salary = employee_df.select(sum('salary')).first()[0]
print("Total Salary:", total_salary)

# Group by department and calculate the average salary
avg_salary_by_department = employee_df.groupBy('department').agg(avg('salary'))
avg_salary_by_department.show()

# Import necessary libraries and create a SparkSession
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.appName("DataFrameAggregation").getOrCreate()

# Sample DataFrame
data = [("Alice", "Sales", 5000),
        ("Bob", "IT", 6000),
        ("Charlie", "Sales", 4500),
        ("David", "IT", 5500),
        ("Eva", "HR", 7000)]

columns = ["EmployeeName", "Department", "Salary"]

df = spark.createDataFrame(data, columns)

# Show the DataFrame
df.show()

from pyspark.sql.functions import count

df.select(count("*").alias("TotalEmployees")).show()

from pyspark.sql.functions import sum

df.select(sum("Salary").alias("TotalSalary")).show()

grouped_df = df.groupBy("Department").agg(sum("Salary").alias("TotalSalary"))
grouped_df.show()

from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.appName("AggregationExample").getOrCreate()

# Sample data
data = [(1, "Alice", 100),
        (2, "Bob", 200),
        (3, "Charlie", 150),
        (4, "David", 300),
        (5, "Eva", 250)]

# Create a DataFrame
df = spark.createDataFrame(data, ["id", "name", "salary"])

# Aggregation example: Calculate the sum, average, and count of salaries
aggregated_df = df.agg({"salary": "sum", "salary": "avg", "salary": "count"})

# Renaming the columns for clarity
aggregated_df = aggregated_df.withColumnRenamed("avg(salary)", "average_salary") \
                             .withColumnRenamed("count(salary)", "count_of_employees")

# Show the result
aggregated_df.show()


#Multiple aggregrations on Grouped Data more examples 
