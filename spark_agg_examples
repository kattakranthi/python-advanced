from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# Create SparkSession
spark = SparkSession.builder.getOrCreate()

# Load employee DataFrame
employee_df = spark.read.csv('employee.csv', header=True, inferSchema=True)

# Display the employee DataFrame
employee_df.show()

# Count the number of employees
employee_count = employee_df.count()
print("Employee Count:", employee_count)

# Calculate the average salary
average_salary = employee_df.select(avg('salary')).first()[0]
print("Average Salary:", average_salary)

# Calculate the maximum salary
max_salary = employee_df.select(max('salary')).first()[0]
print("Maximum Salary:", max_salary)

# Calculate the minimum salary
min_salary = employee_df.select(min('salary')).first()[0]
print("Minimum Salary:", min_salary)

# Calculate the sum of salaries
total_salary = employee_df.select(sum('salary')).first()[0]
print("Total Salary:", total_salary)

# Group by department and calculate the average salary
avg_salary_by_department = employee_df.groupBy('department').agg(avg('salary'))
avg_salary_by_department.show()

# Import necessary libraries and create a SparkSession
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.appName("DataFrameAggregation").getOrCreate()

# Sample DataFrame
data = [("Alice", "Sales", 5000),
        ("Bob", "IT", 6000),
        ("Charlie", "Sales", 4500),
        ("David", "IT", 5500),
        ("Eva", "HR", 7000)]

columns = ["EmployeeName", "Department", "Salary"]

df = spark.createDataFrame(data, columns)

# Show the DataFrame
df.show()

from pyspark.sql.functions import count

df.select(count("*").alias("TotalEmployees")).show()

from pyspark.sql.functions import sum

df.select(sum("Salary").alias("TotalSalary")).show()

grouped_df = df.groupBy("Department").agg(sum("Salary").alias("TotalSalary"))
grouped_df.show()

#Multiple aggregrations on Grouped Data more examples 
